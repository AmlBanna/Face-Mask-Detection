{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:27:29.527701Z","iopub.execute_input":"2025-10-26T19:27:29.528407Z","iopub.status.idle":"2025-10-26T19:27:31.682681Z","shell.execute_reply.started":"2025-10-26T19:27:29.528384Z","shell.execute_reply":"2025-10-26T19:27:31.681931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport random\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom xml.etree import ElementTree as ET\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nTF_AVAILABLE = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:27:31.684018Z","iopub.execute_input":"2025-10-26T19:27:31.684345Z","iopub.status.idle":"2025-10-26T19:27:44.551374Z","shell.execute_reply.started":"2025-10-26T19:27:31.684326Z","shell.execute_reply":"2025-10-26T19:27:44.550794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\nDATA_DIR = Path('/kaggle/input/face-mask-detection')\nIMAGES_DIR = DATA_DIR / 'images'\nANNOTS_DIR = DATA_DIR / 'annotations'\n\nprint(\"\\nLooking for dataset folders under:\", DATA_DIR.resolve())\nfor p in [IMAGES_DIR, ANNOTS_DIR]:\n    print(f\" - {p} -> exists? {p.exists()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:27:44.552079Z","iopub.execute_input":"2025-10-26T19:27:44.552486Z","iopub.status.idle":"2025-10-26T19:27:44.557548Z","shell.execute_reply.started":"2025-10-26T19:27:44.552467Z","shell.execute_reply":"2025-10-26T19:27:44.556755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\ndef parse_voc_xml(xml_path):\n    \"\"\"\n    Parse a single Pascal VOC XML file.\n    Returns list of dicts: [{'filename':..., 'xmin':..., 'ymin':..., 'xmax':..., 'ymax':..., 'class':...}, ...]\n    \"\"\"\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    filename = root.findtext('filename')\n    objects = []\n    for obj in root.findall('object'):\n        cls = obj.findtext('name')\n        bndbox = obj.find('bndbox')\n        xmin = int(float(bndbox.findtext('xmin')))\n        ymin = int(float(bndbox.findtext('ymin')))\n        xmax = int(float(bndbox.findtext('xmax')))\n        ymax = int(float(bndbox.findtext('ymax')))\n        objects.append({\n            'filename': filename,\n            'xmin': xmin,\n            'ymin': ymin,\n            'xmax': xmax,\n            'ymax': ymax,\n            'class': cls\n        })\n    return objects\n\n# Collect all annotations\nannots_dir = ANNOTS_DIR\nrecords = []\nif annots_dir.exists():\n    xml_files = sorted(list(annots_dir.glob(\"*.xml\")))\n    print(f\"Found {len(xml_files)} XML files in {annots_dir}\")\n    for xml in xml_files:\n        records.extend(parse_voc_xml(xml))\nelse:\n    print(f\"Annotations folder not found: {annots_dir}\")\n    \n# Create DataFrame\ndf = pd.DataFrame(records, columns=['filename','xmin','ymin','xmax','ymax','class'])\nprint(\"DataFrame shape:\", df.shape)\ndisplay(df.head())\n\n# Basic checks\nprint(\"\\nMissing values per column:\")\nprint(df.isnull().sum())\n\n# Count unique images & per-class counts\nif not df.empty:\n    print(\"\\nUnique images:\", df['filename'].nunique())\n    print(\"Objects per class:\")\n    print(df['class'].value_counts())\nelse:\n    print(\"No annotation records were parsed â€” check your annotations folder and XML format.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:27:44.559300Z","iopub.execute_input":"2025-10-26T19:27:44.559541Z","iopub.status.idle":"2025-10-26T19:27:46.429210Z","shell.execute_reply.started":"2025-10-26T19:27:44.559519Z","shell.execute_reply":"2025-10-26T19:27:46.428478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"invalid = df[(df['xmax'] <= df['xmin']) | (df['ymax'] <= df['ymin'])]\nprint(f\" Invalid boxes found: {len(invalid)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:27:46.429966Z","iopub.execute_input":"2025-10-26T19:27:46.430268Z","iopub.status.idle":"2025-10-26T19:27:46.436843Z","shell.execute_reply.started":"2025-10-26T19:27:46.430244Z","shell.execute_reply":"2025-10-26T19:27:46.436194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nclass_counts = df['class'].value_counts()\n\nplt.figure(figsize=(8,5))\nplt.bar(class_counts.index, class_counts.values)\nplt.title(\"Class Distribution (Number of Objects per Class)\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:27:46.437475Z","iopub.execute_input":"2025-10-26T19:27:46.437774Z","iopub.status.idle":"2025-10-26T19:27:46.629525Z","shell.execute_reply.started":"2025-10-26T19:27:46.437753Z","shell.execute_reply":"2025-10-26T19:27:46.628864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplt.pie(class_counts.values,\n        labels=class_counts.index,\n        autopct='%1.1f%%',\n        startangle=140,\n        colors=[\"#66c2a5\", \"#fc8d62\", \"#8da0cb\"])\nplt.title(\"Class Distribution (%)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:27:46.630193Z","iopub.execute_input":"2025-10-26T19:27:46.630408Z","iopub.status.idle":"2025-10-26T19:27:46.737666Z","shell.execute_reply.started":"2025-10-26T19:27:46.630391Z","shell.execute_reply":"2025-10-26T19:27:46.736943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\ndef show_image_with_boxes(filename):\n    img_path = os.path.join(IMAGES_DIR, filename)\n    image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n    boxes = df[df['filename'] == filename]\n    \n    for _, row in boxes.iterrows():\n        cv2.rectangle(image, (row['xmin'], row['ymin']), (row['xmax'], row['ymax']),\n                      (0, 255, 0), 2)\n        cv2.putText(image, row['class'], (row['xmin'], row['ymin'] - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n    \n    plt.figure(figsize=(8,8))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n\nshow_image_with_boxes(df['filename'].iloc[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:29:12.691311Z","iopub.execute_input":"2025-10-26T19:29:12.691645Z","iopub.status.idle":"2025-10-26T19:29:12.970905Z","shell.execute_reply.started":"2025-10-26T19:29:12.691623Z","shell.execute_reply":"2025-10-26T19:29:12.970153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport random\nimport os\n\nsample_files = random.sample(list(df['filename'].unique()), 5)\n\ndef show_image_with_boxes(filename):\n    img_path = os.path.join(IMAGES_DIR, filename)\n    image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n    boxes = df[df['filename'] == filename]\n    \n    for _, row in boxes.iterrows():\n        color = (0, 255, 0) if row['class'] == 'with_mask' else \\\n                (255, 0, 0) if row['class'] == 'without_mask' else \\\n                (255, 165, 0)\n        cv2.rectangle(image, (row['xmin'], row['ymin']), (row['xmax'], row['ymax']), color, 2)\n        cv2.putText(image, row['class'], (row['xmin'], row['ymin'] - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n    \n    plt.figure(figsize=(6,6))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.title(filename)\n    plt.show()\n\nfor file in sample_files:\n    show_image_with_boxes(file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:29:25.162436Z","iopub.execute_input":"2025-10-26T19:29:25.163028Z","iopub.status.idle":"2025-10-26T19:29:26.068293Z","shell.execute_reply.started":"2025-10-26T19:29:25.163003Z","shell.execute_reply":"2025-10-26T19:29:26.067459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n\nIMG_SIZE = 128  \nfaces = []\nlabels = []\n\nprint(\"Starting to load and crop images...\")\nfor idx, row in df.iterrows():\n    img_path = IMAGES_DIR / row['filename']\n    if not img_path.exists():\n        continue \n    \n    img = cv2.imread(str(img_path))\n    if img is None:\n        continue\n    \n    x1, y1, x2, y2 = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n    face = img[y1:y2, x1:x2]\n    \n    # skip invalid boxes\n    if face.size == 0:\n        continue\n    \n    face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n    \n    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n    \n    face = preprocess_input(face)\n    \n    faces.append(face)\n    labels.append(row['class'])\n\nprint(f\"Loaded {len(faces)} cropped faces.\")\n\nX = np.array(faces, dtype=\"float32\")\ny = np.array(labels)\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\ny_cat = to_categorical(y_encoded)\n\nprint(\"Classes found:\", le.classes_)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_cat, test_size=0.2, stratify=y_cat, random_state=42\n)\n\nprint(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\nprint(f\"X shape: {X_train.shape[1:]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:29:37.018337Z","iopub.execute_input":"2025-10-26T19:29:37.018846Z","iopub.status.idle":"2025-10-26T19:30:38.962511Z","shell.execute_reply.started":"2025-10-26T19:29:37.018821Z","shell.execute_reply":"2025-10-26T19:30:38.961813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nINIT_LR = 1e-4     \nEPOCHS = 10       \nBS = 32            \n\nbaseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n                        input_tensor=Input(shape=(128, 128, 3)))\n\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(4, 4))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(128, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(3, activation=\"softmax\")(headModel)\n\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\nfor layer in baseModel.layers:\n    layer.trainable = False\n\nopt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\nmodel.compile(loss=\"categorical_crossentropy\",\n              optimizer=opt,\n              metrics=[\"accuracy\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:31:14.983067Z","iopub.execute_input":"2025-10-26T19:31:14.983357Z","iopub.status.idle":"2025-10-26T19:31:19.209454Z","shell.execute_reply.started":"2025-10-26T19:31:14.983336Z","shell.execute_reply":"2025-10-26T19:31:19.208660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aug = ImageDataGenerator(\n    rotation_range=20,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:31:42.667283Z","iopub.execute_input":"2025-10-26T19:31:42.667808Z","iopub.status.idle":"2025-10-26T19:31:42.671630Z","shell.execute_reply.started":"2025-10-26T19:31:42.667785Z","shell.execute_reply":"2025-10-26T19:31:42.670899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"[INFO] Training the model...\")\nH = model.fit(\n    aug.flow(X_train, y_train, batch_size=BS),\n    steps_per_epoch=len(X_train) // BS,\n    validation_data=(X_test, y_test),\n    validation_steps=len(X_test) // BS,\n    epochs=EPOCHS\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:31:46.241562Z","iopub.execute_input":"2025-10-26T19:31:46.242265Z","iopub.status.idle":"2025-10-26T19:33:03.969423Z","shell.execute_reply.started":"2025-10-26T19:31:46.242242Z","shell.execute_reply":"2025-10-26T19:33:03.968845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"[INFO] Evaluating the model...\")\npredIdxs = model.predict(X_test, batch_size=BS)\npredIdxs = np.argmax(predIdxs, axis=1)\n\ny_test_labels = np.argmax(y_test, axis=1)\n\nprint(classification_report(y_test_labels, predIdxs,\n                            target_names=['mask_incorrect', 'with_mask', 'without_mask']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:33:19.599317Z","iopub.execute_input":"2025-10-26T19:33:19.599705Z","iopub.status.idle":"2025-10-26T19:33:27.191375Z","shell.execute_reply.started":"2025-10-26T19:33:19.599684Z","shell.execute_reply":"2025-10-26T19:33:27.190796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred_classes))\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:34:01.364067Z","iopub.execute_input":"2025-10-26T19:34:01.364841Z","iopub.status.idle":"2025-10-26T19:34:02.595235Z","shell.execute_reply.started":"2025-10-26T19:34:01.364816Z","shell.execute_reply":"2025-10-26T19:34:02.594463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"mask_detector_v1.h5\")\nprint(\" Model saved as mask_detector_v1.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:34:07.722595Z","iopub.execute_input":"2025-10-26T19:34:07.723107Z","iopub.status.idle":"2025-10-26T19:34:08.043817Z","shell.execute_reply.started":"2025-10-26T19:34:07.723086Z","shell.execute_reply":"2025-10-26T19:34:08.043017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:34:30.885263Z","iopub.execute_input":"2025-10-26T19:34:30.885817Z","iopub.status.idle":"2025-10-26T19:34:31.407009Z","shell.execute_reply.started":"2025-10-26T19:34:30.885793Z","shell.execute_reply":"2025-10-26T19:34:31.406294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.path.exists(\"haarcascade_frontalface_default.xml\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:34:34.828070Z","iopub.execute_input":"2025-10-26T19:34:34.828706Z","iopub.status.idle":"2025-10-26T19:34:34.833401Z","shell.execute_reply.started":"2025-10-26T19:34:34.828679Z","shell.execute_reply":"2025-10-26T19:34:34.832651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
